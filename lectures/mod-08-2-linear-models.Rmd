---
title: "Linear models"
subtitle: "<br><br> Behavioral Data Science in R"
author: "Derek Powell & Nicholas Duran"
output:
  xaringan::moon_reader:
    css: ["xaringan-themer.css", "slides.css"]
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      highlightStyle: solarized-light
      countIncrementalSlides: false
---

```{r child = "setup.Rmd"}
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
# library(xaringanthemer)
knitr::opts_chunk$set(dev.args = list(bg = 'transparent'), echo=F)
```


## Models as functions

.hand[
Remember we said the data is a _function_ of the model and noise
]

$$\text{data} = f(\text{model, noise})$$
--

- In principle, that function $f$ can be almost anything. 
- In practice, we can often do a good job of describing the data by defining $f$ to be some linear function---then we have a __linear model__.
---

## Lines

.pull-left[
The simplest kind of linear model is a line!
$$y = mx + b $$

- m is the slope
- b is the y-intercept
]

--

.pull-right[
In statistics we like to use greek letters

$$ y = \alpha + \beta x$$
- $\alpha$ is the __intercept__
- $\beta$ is the __coefficient__ for the predictor $x$

$\alpha$ and $\beta$ are our __parameters__
]

---

## Predicting $y$ from $x$

Say we have some points on $x$ and $y$ and we want to create a model of $y$ based on $x$.

.pull-left-wide[

```{r, echo=F, out.width="150%"}


standardize <- function(x){
  (x-mean(x))/sd(x)
}


N <- 10

set.seed(13312)

dfsim <- tibble(
  x = rnorm(N, 2, 2),
  y = -1 + 2/3*x + rnorm(N, 0, 2)
) %>% 
  # mutate(x = x*2, y = y*2) %>% 
  bind_rows(
    tribble(
      ~x, ~y,
      3, 1,
      6, 3
    )
  )

dfsim %>% 
  ggplot(aes(x=x, y=y)) + 
  geom_point() +
  theme(aspect.ratio=1) +
  theme_bw(base_size = 20) +
  theme(
    panel.grid=element_blank(), 
    aspect.ratio=1
    ) +
  scale_x_continuous(breaks = c(-2, 0, 2, 4, 6, 8))
```
]
--
.pull-right-narrow[
Suppose we wanted to predict the value of $y$ when $x = 2$. 

What would you predict? 
]

---

## Predicting visually

First, we can do this visually:

.pull-left-wide[

```{r, echo=F, out.width="150%"}

dfsim %>% 
  ggplot(aes(x=x, y=y)) + 
  annotate("rect", xmin=1.5, xmax=2.5, ymin=-Inf, ymax=Inf, alpha=.5, fill="grey") +
  geom_point() +
  geom_point(aes(x=2, y = .5), size=3, color="blue") +
  # geom_segment(aes(x = 2, xend=2, y=.5, yend=-Inf), linetype="dashed", color="red") +
  theme(aspect.ratio=1) +
  theme_bw(base_size = 20) +
  theme(
      panel.grid=element_blank(), 
      aspect.ratio=1
  ) +
  scale_x_continuous(breaks = c(-2, 0, 2, 4, 6, 8))
```
]

.pull-right-narrow[
Suppose we wanted to predict the value of $y$ when $x = 2$. 

What would you predict? 
]

---

## Predicting visually



.pull-left-wide[

```{r, echo=F, out.width="150%"}

dfsim %>% 
  ggplot(aes(x=x, y=y)) + 
  geom_point() +
  geom_abline(intercept = -1.1, slope = 2/3.1, color="blue") + # slightly off
  theme(aspect.ratio=1) +
  theme_bw(base_size = 20) +
  theme(
      panel.grid=element_blank(), 
      aspect.ratio=1
  ) +
  scale_x_continuous(breaks = c(-2, 0, 2, 4, 6, 8))
```
]

.pull-right-narrow[
Now what if we wanted to be able to this for any value of $x$? If you string all those points together, you get a line.
]

---

## Fitting a line to two points

How can we actually fit these lines without just drawing them based on sight?

.pull-left-wide[

Say we had only two points: (3, 1) and (6, 3)

```{r, echo=F, out.width="80%", fig.align="center"}
dfsim %>%
  tail(2) %>% 
  ggplot(aes(x=x, y=y)) + 
  geom_point() +
  # geom_abline(intercept = -1.1, slope = 2/3.1, color="blue") + # slightly off
  theme(aspect.ratio=1) +
  theme_bw(base_size = 20) +
  theme(
      panel.grid=element_blank(), 
      aspect.ratio=.5
  ) +
  scale_x_continuous(breaks = c(-2, 0, 2, 4, 6, 8)) +
  xlim(0, 8) +
  ylim(0, 4)
```
]

--

.pull-right-narrow[


Well if you remember your high school algebra, we can solve this system of equations:

$$
\begin{align}
\alpha + \beta \cdot 3 & = 1 \\
\alpha + \beta \cdot 6 & = 3
\end{align}
$$
]

---

## Finding parameters from two points

$$
\begin{align}
\alpha + \beta \cdot 3 & = 1 \\
\alpha + \beta \cdot 6 & = 3
\end{align}
$$

--

Working through the steps:

$$
\begin{align}
\alpha + \beta \cdot 3 & = 1 \\
\beta \cdot 3 & = 2
\end{align}
$$

--

$$
\begin{align}
\alpha + \beta \cdot 3 & = 1 \\
\beta & = 2/3
\end{align}
$$

--

And then by substitution you can find $\alpha = -1$.

So our __parameters__ are $\beta = 2/3$ and $\alpha = -1$ 

---

## Fitting lines to many points

.hand[No line is perfect, how can we tell which line is best?]

```{r echo=F}
dfsim %>% 
  ggplot(aes(x=x, y=y)) + 
  geom_point() +
  geom_abline(intercept = -2, slope = 1, color="red") + # slightly off
  geom_abline(intercept = -1.1, slope = 2/3.1, color="blue") + # slightly off
  geom_abline(intercept = 0, slope = .5, color="purple") + # slightly off
  theme(aspect.ratio=1) +
  theme_bw(base_size = 20) +
  theme(
      panel.grid=element_blank(), 
      aspect.ratio=1
  ) +
  scale_x_continuous(breaks = c(-2, 0, 2, 4, 6, 8))
```

---

## Residuals

These grey lengths represent the __residuals__, the differences between the predicted value (line) and each observation (point).

```{r echo=F}

dfsim %>% 
    mutate(predy = x*2/3 + -1) %>% 
  ggplot(aes(x=x, y=y)) +
  geom_abline(slope = 2/3, intercept = -1, color="blue" ) +
  geom_segment(aes(xend=x, yend=predy), color="grey") +
  geom_point() +
  # theme(aspect.ratio=1)
  # ggplot(aes(x=x, y=y)) + 
  # geom_point() +
  # geom_abline(intercept = -2, slope = 1, color="red") + # slightly off
  # geom_abline(intercept = -1.1, slope = 2/3.1, color="blue") + # slightly off
  # geom_abline(intercept = 0, slope = .5, color="purple") + # slightly off
  # theme(aspect.ratio=1) +
  theme_bw(base_size = 20) +
  theme(
      panel.grid=element_blank(),
      # aspect.ratio=1
  ) +
  scale_x_continuous(breaks = c(-2, 0, 2, 4, 6, 8))

# rand10 %>% 
#   mutate(predy = x) %>% 
#   ggplot(aes(x=x, y=y)) + 
#   geom_abline(slope = 1, intercept = 0, color="blue" ) +
#   geom_segment(aes(xend=x, yend=predy), color="grey") +
#   geom_point() +
#   theme(aspect.ratio=1)
```

--

Our goal is to somehow minimize these residuals. Can you think of a mathematical way we could do that?

---

## Loss functions

One thing we could try to do is minimize the sum of the absolute values. 

$$ |e_0| + |e_1| + |e_2| + ... + |e_n| $$

--

This is a legitimate approach, but it has some issues. A more common approach is to minimize the __sum of the squared residuals__:

$$ e_0^2 + e_1^2 + e_2^2 + ... + e_n^2 = \sum_i e_i^2$$

When we do this, this is called __least squares regression__

---

## Minimizing loss functions


- We want to find the parameters that will minimize this __loss function__ $L$:

$$\underset{\alpha, \beta}{\mathrm{argmin}} \, L(x, y, \alpha, \beta)$$ 

- $x$ and $y$ are our data, so we are looking to minimize this with respect to our __parameters__ $\alpha$ and $\beta$. 

- We write our regression equation as:

$$\hat{y_i} = \alpha + \beta x_i$$

- Our residuals are the difference between $y$ and $\hat{y}$, so our loss function is

$$L(x, y, \alpha, \beta) = \sum_i (y_i-\hat{y_i})^2$$

<!-- We'll skip the calculus and linear algebra involved, but one reason to use least squares regression is that it is easy to minimize this function and find our best parameters. -->
---

## Maximum likelihood

__Maximum likelihood__ approach asks: What parameter values would _maximize the likelihood_ of observing the data we observed?

- Minimizing least squares is also equivalent to the maximum likelihood solution 
- If we assume that the residuals come from a normal distribution.

--

.pull-left-narrow[

- The __normal distribution__ is the classic bell-shaped curve.
- This is a plot of its density, which we can read like the other histograms and density plots we've seen

]

.pull-right-wide[

```{r, fig.height="50%"}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + ylab("") +
  scale_y_continuous(breaks = NULL) +
  theme_bw()
```

]

---

## Maximum likelihood regression

```{r}
## borrowed from:
## https://stackoverflow.com/a/62553817/8297546
set.seed(234524)
x <- seq(-2.5,2.5,length.out=5)
y <- x*0.5

# x <- x - mean(x)
# y <- y - mean(y)

df <- data.frame(x, y)

# For every row in `df`, compute a rotated normal density centered at `y` and shifted by `x`
curves <- lapply(seq_len(NROW(df)), function(i) {
  mu <- df$y[i]
  range <- mu + c(-3, 3)
  seq <- seq(range[1], range[2], length.out = 100)
  data.frame(
    x = 1 * dnorm(seq, mean = mu) + df$x[i],
    y = seq,
    grp = i
  )
})

# Combine above densities in one data.frame
curves <- do.call(rbind, curves)

sim_obs_data <- tibble(
  x = sample(seq(-3,3,.1), size=200, replace=T),
  y = x*.5 + rnorm(200, 0, 1)
)

ggplot(mapping=aes(x, y)) +
  # geom_line() +
  geom_abline(slope=.5, intercept=0) +
  # The path draws the curve
  # geom_path(data = curves, aes(x=x, group = grp)) +
  # The polygon does the shading. We can use `oob_squish()` to set a range.
  geom_polygon(data = curves, aes(x=x, group = grp), alpha=.33, fill="blue") +
  geom_point(data = sim_obs_data) +
  theme_bw() +
  theme(panel.grid = element_blank())
```


---

## Central Limit Theorem

.hand[Why make a model that assumes the errors are normally distributed?]

--
- Many quantities end up approximately normally distributed
- This is due to the __Central Limit Theorem__

__Central Limit Theorem __

If $x_1, x_2, ..., x_n$ are random samples from a population with overall mean $\mu$ and finite variance $\sigma^2$, then the limiting form of the distribution of these samples $X$ is a normal distribution

<!-- See chapter 13 of Intro to modern statistics for more  -->
---

class: middle

# Example: Fitting a model of height and weight

---

## Height and weight data

```{r, echo=T}
data("Howell1")
head(Howell1)
```



---