---
title: "LAB 09 I - Evaluating the General Social Survey (GSS)"
# author:
  # - Jessica Kosie^[Authored modifications to original code]
  # - Nicholas Duran^[Original author of exercise]
# date: "`r Sys.Date()`"
editor: 
  markdown: 
    wrap: sentence
---

![](img/mauro-mora-31-pOduwZGE-unsplash.jpg){fig-align="center"}

The General Social Survey (GSS) gathers data on contemporary American society in order to monitor and explain trends and constants in attitudes, behaviors, and attributes.
Hundreds of trends have been tracked since 1972.

In addition, since the GSS adopted questions from earlier surveys, trends can be followed for up to 70 years.

The GSS contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest.

Among the topics covered are civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events.

In this assignment we analyze data from the 2016 GSS, using it to do some predictions.

::: callout-important
**LEARNING OBJECTIVES**

-   More practice with data manipulation (e.g., releveling and reordering factor levels, and handling NA values)

-   Develop skills in data splitting for machine learning

-   Implement feature engineering in predictive modeling: Use the `recipes` package

-   Build and evaluate logistic regression models using `tidymodels`: Construct a logistic regression model, specify it with `tidymodels` syntax, and use workflows to combine pre-processing and model fitting steps.

-   Apply and interpret cross-validation and model comparison techniques: Perform k-fold cross-validation, collect performance metrics like ROC and accuracy, and compare different model structures to make informed decisions about model performance.
:::

# Getting started

Go to the course Posit Cloud Workspace and locate the R Studio Project for this Module, which should be named `Module 9`.

First, open the R Markdown document `mod-09-lab.Rmd` and **Knit** it.

Make sure it compiles without errors and you can preview the output within the Viewer Pane.

## Warm up

Before we introduce the data, let's warm up with a simple exercise.

-   Update the YAML, changing the author name to your name, and **Knit** the document.

## Packages

We'll use the **tidyverse** package for much of the data wrangling and visualization, the **tidymodels** package for modeling and prediction, and the data lives in the **dsbox** package.

These packages are already installed for you.

You can load them by running the following in your Console:

```{r}
#| label: load-packages
#| message: false
#| eval: true

library(tidyverse)
library(tidymodels)
library(dsbox)
```

## Data

The data can be found in the **dsbox** package, and it's called `gss16`.
Since the dataset is distributed with the package, we don't need to load it separately; it becomes available to us when we load the package.
You can find out more about the dataset by inspecting its documentation, which you can access by running `?gss16` in the Console or using the Help menu in RStudio to search for `gss16`.

You can also find this information [here](https://rdrr.io/github/rstudio-education/dsbox/man/gss16.html).

# Exercises

Take turns answering the exercises on one of your computers.

You don't have to switch at each exercise, you can find your a cadence that works for your team and stick to it.

Before the end of class, please copy your answers over answers to your teammates.

## Scientific research

In this section we're going to build a model to predict whether someone agrees or doesn't agree with the following statement:

> Even if it brings no immediate benefits, scientific research that advances the frontiers of knowledge is necessary and should be supported by the federal government.

The responses to the question on the GSS about this statement are in the `advfront` variable.

## Part 1: Data wrangling

### Exercise 1

-   Re-level the `advfront` variable such that it has two levels: `Strongly agree` and "`Agree"` combined into a new level called `agree` and the remaining levels (except `NA`s) combined into `"Not agree"`.

::: callout-warning
It's important that you don't recode the NAs, just the remaining levels.
:::

-   Then, re-order the levels in the following order: `"Agree"` and `"Not agree"`.

-   Finally, `count()` how many times each new level appears in the `advfront` variable.

::: callout-tip
To answer Exercise 1, you're going to use `mutate`, `case_when`, and `factor` or `fct_relevel` at some point.
:::

### Exercise 2

-   Combine the levels of the `polviews` variable such that levels that have the word "liberal" in them are lumped into a level called `"Liberal"` and those that have the word conservative in them are lumped into a level called `"Conservative"`.

::: callout-tip
You can do this in various ways.
One option is to use the `str_detect()` function to detect the existence of words like liberal or conservative.
Note that these sometimes show up with lowercase first letters and sometimes with upper case first letters.
To detect either in the `str_detect()` function, you can use "\[Ll\]iberal" and "\[Cc\]onservative".
But feel free to solve the problem however you like, this is just one option!
:::

-   Then, re-order the levels in the following order: `"Conservative"` , `"Moderate"`, and `"Liberal"`.

-   Finally, `count()` how many times each new level appears in the `polviews` variable.

::: callout-tip
To answer Exercise 2, in addition to the functions already mentioned, you're going to use `mutate` and `factor` or `fct_relevel` at some point.
:::

### Exercise 3

-   Create a new data frame called `gss16_advfront` that includes the variables `advfront`, `educ`, `polviews`, and `wrkstat`.

-   Then, use the `drop_na()` function to remove rows that contain `NA`s from this new data frame.
    Sample code is provided below.

```{r}
#| eval: false

gss16_advfront <- gss16 %>%
  select(___, ___, ___, ___) %>%
  drop_na()
```

## Part 2: Data spending

### Exercise 4

-   Split the data into training (75%) and testing (25%) data sets.
    Make sure to set a seed before you do the `initial_split()`.
    Call the training data `gss16_train` and the testing data `gss16_test`.

-   Use these specific names below to make it easier to follow the rest of the instructions.

-   Set the seed to the value "123" so we all get the same result

```{r}
#| eval: false

___ # do something here to set the seed

gss16_split <- ___
gss16_train <- ___
gss16_test  <- ___
```

## Part 3: Feature engineering

### Exercise 5

In the lecture I introduced recipes briefly, here is your opportunity to extend and learn on your own.

See the sample code provided below and complete it. **Note** that the recipe object you're building is called `gss16_rec_1`. 
Preprocessing/feature engineering steps:

- First declare you're creating a recipe using `recipe` and fill in the missing arguments

- Next, for the the `wrkstat` variable, you're going to pool values in `wrkstat` that occur less than 10% of the time and call them `Other`. You will use the default `step_other()` function to do so. 

- Next, for all nominal (categorical) variables, you're going to create dummy variables. **Note** that using `-all_outcomes()` ensures that the target variable (outcome) is not accidentally included when creating dummy variables. Your focus should only be on predictors.  

```{r}
#| eval: false

gss16_rec_1 <- recipe(___ ~ ___, data = ___) %>%
  step_other(___, threshold = ___, ___ = "Other") %>%
  step_dummy(___, -all_outcomes())
```

::: callout-note
Remember: "Recipes" use dplyr-like pipeable sequences of feature engineering steps to get your data ready for modeling.
To learn more about what each step does, feel free to explore with `?step_other` and `?step_dummy`.
To learn more about recipes in general, visit <https://recipes.tidymodels.org//articles/recipes.html>
:::

## Part 4: Buld a workflow

### Exercise 6

Specify a logistic regression model using `"glm"` as the engine.
Name this specification `gss16_spec`.

Do this the tidymodels way.

```{r}
#| eval: false

gss16_spec <- ___ %>%
  ___
```

### Exercise 7

Build a workflow that uses the recipe you defined (`gss16_rec`) and the model you specified (`gss16_spec`).
Name this workflow `gss16_wflow_1`.

```{r}
#| eval: false

gss16_wflow_1 <- ___ %>% # first declare what you're doing
  ___ %>% # add something
  ___ # add something
```

## Part 5: Model fit with cross-validation

### Exercise 8

Perform 5-fold cross validation.
specifically,

-   #1: split the training data into 5 folds (don't forget to set a seed: set it as `123`),

-   #2: apply the workflow you defined earlier to the folds with `fit_resamples()`, and

-   #3: `collect_metrics()` and comment on the consistency of metrics across folds (you can get the area under the ROC curve and the accuracy for each fold by setting `summarize = FALSE` in `collect_metrics()`)

-   #4: report the average area under (auc) for the ROC and the accuracy for all cross validation folds `collect_metrics()`

```{r}
#| eval: false

#1:
___ # do something here to set the seed

gss16_folds < ___

#2:
gss16_fit_rs_1 <- ___ %>%
 # here you fit the resampled data

#3:
# here you collect the metrics with summarize set at false

#4
# here you collect the metrics, but now showing the average
```

## Part 6: Model comparison

### Exercise 9.

Now, try a different, simpler model: predict `advfront` from only `polviews` and `educ`.
Specifically,

-   #1: update the recipe to reflect this simpler model specification (and name it `gss16_rec_2`),

-   #2: redefine the workflow with the new recipe (and name this new workflow `gss16_wflow_2`),

-   #3: perform cross validation, and

-   #4: report the average area under the ROC curve and the accuracy for all cross validation folds `collect_metrics()`.

```{r}
#| eval: false

#1:
gss16_rec_2 < gss16_rec_1 %>% # just modify the original recipe
  # there is something very simple you can do here with `step_rm`

#2:
gss16_wflow_2 <- ___ %>%
 # setting up a new workflow, but you can copy/paste from above, you're just swapping out to the new recipe 

#3:
# don't forget to set the seed

#4
# collect the metrics like you did before
```

### Exercise 10

Comment on which model performs better (one including `wrkstat`, model 1, or the one excluding `wrkstat`, model 2) on the training data based on area under curve for the ROC.

::: callout-note
If not obvious from the lectures, this sort of model comparison is what cross-validation allows!
:::

## Part 7: Get test set performance metrics

### Exercise 11

Take the best performing model (see Exercise 10) and use the corresponding workflow (either `gss16_wflow_1` or `gss16_wflow_2`) and fit it on all the training data (no resampling). Then, take this model and fit it on the test data. Provide some metrics of test data performance. 

At the very least, performance metrics like:

- **plot** the ROC curve for the predictions
- calculate the area under curve for the ROC

Once you've run this, answer the following questions: What would it mean if the fit on the testing data was much worse than the fit on the training data? Explain your reasoning.

## Part 8: Starting over w/ a decision tree

::: callout-note
"Our greatest weakness lies in giving up. The most certain way to succeed is always to try just one more time." \~ Thomas A. Edison
:::

### Exercise 12

In this final challenge, lets build a completely new classification model using this data.

**Using my lecture materials, I want you to:**

-   set up the specs for a decision tree, add it to a workflow

-   then, set up to do a ten fold cross-validation, fitting on the resamples (you'll use your workflow for this)

-   take the resulting object, and collect performance metrics on it

-   and finally, you should have gotten a single `roc_auc` value for the decision tree.
    Comparing with your logistic regression `roc_auc`, what model would you recommend as the best classifier?

### You're done

<!-- ## Harassment at work -->

<!-- In 2016, the GSS added a new question on harassment at work. -->

<!-- The question is phrased as the following. -->

<!-- > Over the past five years, have you been harassed by your superiors or co-workers at your job, for example, have you experienced any bullying, physical or psychological abuse? -->

<!-- Answers to this question are stored in the `harass5` variable in our dataset. -->

<!-- ### Exercise 12 -->

<!-- Create a subset of the data that only contains `Yes` and `No` answers for the harassment question.  -->

<!-- - How many responses chose each of these answers? -->

<!-- ### Exercise 13 -->

<!-- Describe how bootstrapping can be used to estimate the proportion of Americans who have been harassed by their superiors or co-workers at their job. -->

<!-- ### Exercise 14 -->

<!-- Calculate a 95% bootstrap confidence interval for the proportion of Americans who have been harassed by their superiors or co-workers at their job. Interpret this interval in context of the data. -->

<!-- ### Exercise 15 -->

<!-- Would you expect a 90% confidence interval to be wider or narrower than the interval you calculated above? Explain your reasoning. -->

Now go back through your write up to make sure you've answered all questions and all of your R chunks are properly labelled.

::: callout-note
**SUBMITTING YOUR WORK**

Once you decide as a team that you're done with this lab, change the YAML `output` from `html_output` to `pdf_output.` Now knit the document to produce a final PDF file.
Make sure all team members have access to the PDF and each person needs to upload the PDF as a Canvas assignment.
:::
