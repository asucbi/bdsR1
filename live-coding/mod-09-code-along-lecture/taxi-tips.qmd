---
title: "Prediction w/ tidyModels" 
author: "Nick Duran"
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    toc-depth: 2
    toc-location: right
    self-contained: true
    # grid:
    #   margin-width: 0px   
  #pdf: default
execute:
  echo: fenced
# reference-location: margin
# citation-location: margin
---

```{r setup}
#| include: false
#| file: setup.R
```

`r hexes("tidymodels")`

# PART 1: What are tidyModels 

A modeling framework!

![](img/tm-org.png)

```{r load-tm}
#| message: true
library(tidymodels)
```

### What we will be working on today

- Minimal version of predictive modeling process

### What we will not be working on today but that is in the recorded lectures

- Feature engineering (recipes)

## Machine learning as a predictive modeling process

![](https://imgs.xkcd.com/comics/machine_learning.png){fig-align="center"}

![](img/ml_illustration.jpg){fig-align="center"}

![](img/what_is_ml.jpg){fig-align="center"}

## Your turn

![](img/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

- ### *How are statistics and machine learning related?; How are they different?*

::: {.callout-caution collapse="true"}
## Only open when you have researched your answers for these questions
They are sometimes thought of as capturing "two cultures:" Model first vs. data first; or inference vs. prediction
:::

## The whole game

![](img/whole-game-final-performance.jpg)

### Splitting data (aka your data budget) and building a decision tree

![](img/whole-game-transparent-model-1.jpg)

### Cross validation, i.e., resampling

-   Comparing multiple model types and evaluating to identify best one  

<!-- ![](img/whole-game-transparent-resamples.jpg) -->

![](img/whole-game-transparent-select.jpg)
<!-- ![](img/whole-game-model-1.jpg) -->

<!-- ![](img/whole-game-resamples.jpg) -->

### Creating a final model fit object and doing a final test

![](img/whole-game-final-performance.jpg)

# Data

## Taxi trips in Chicago in 2022

![](img/taxi_spinning.svg)

The city of Chicago releases anonymized trip-level data on taxi trips in the city. We will use a dataset that pulls a sample of 10,000 rides occurring in early 2022.

:::callout-tip
Type `?taxi` to learn more about this dataset, including references.
:::

```{r}
library(tidymodels)
```

```{r}
glimpse(taxi)
```


::: callout-note

`tip`: Whether the rider left a tip. A factor with levels "yes" and "no". 

`distance`: The trip distance, in odometer miles.

`company`: The taxi company, as a factor. Companies that occurred few times were binned as "other".

`local`: Whether the trip started in the same community area as it began. See the source data for community area values.

`dow`: The day of the week in which the trip began, as a factor.

`month`: The month in which the trip began, as a factor.

`hour`: The hour of the day in which the trip began, as a numeric.
:::

### Checklist for predictors

- Is it ethical to use this variable? (Or even legal?)
- Will this variable be available at prediction time?
- Does this variable contribute to explainability?

# PART 2: Data Budget

`r hexes("rsample")`

## Data splitting and spending  

Always have a separate piece of data that can contradict what you believe!

For machine learning, we typically split data into training and test sets:

-   The **training set** is used to estimate model parameters.
-   The **test set** is used to find an independent assessment of model performance. Do not üö´ use the test set during training. The test set is precious üíé

```{r test-train-split}
#| echo: false
#| fig.width: 12
#| fig.height: 3

set.seed(123)
library(forcats)
one_split <- slice(taxi, 1:30) %>% 
  initial_split() %>% 
  tidy() %>% 
  add_row(Row = 1:30, Data = "Original") %>% 
  mutate(Data = case_when(
    Data == "Analysis" ~ "Training",
    Data == "Assessment" ~ "Testing",
    TRUE ~ Data
  )) %>% 
  mutate(Data = factor(Data, levels = c("Original", "Training", "Testing")))
all_split <-
  ggplot(one_split, aes(x = Row, y = fct_rev(Data), fill = Data)) + 
  geom_tile(color = "white",
            linewidth = 1) + 
  # scale_fill_manual(values = splits_pal, guide = "none") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = rel(2)),
        axis.text.x = element_blank(),
        legend.position = "top",
        panel.grid = element_blank()) +
  coord_equal(ratio = 1) +
  labs(x = NULL, y = NULL)
all_split
```

-   Spending too much data in **training** prevents us from computing a good assessment of predictive **performance**.

-   Spending too much data in **testing** prevents us from computing a good estimate of model **parameters**.

## Your turn

![](img/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

* ### *When is a good time to split your data?*

## The initial split 

```{r taxi-split}
set.seed(123)
taxi_split <- initial_split(taxi)
taxi_split
```

::: callout-important
## How much data in training vs testing?

This function uses a good default, but this depends on your specific goal/data. There are more powerful ways of splitting, like stratification, that we will get into later.
:::

::: callout-note
## What is `set.seed()`?

To create that split of the data, R generates "pseudo-random" numbers: while they are made to behave like random numbers, their generation is deterministic give a "seed".

This allows us to reproduce results by setting that seed.

Which seed you pick doesn't matter, as long as you don't try a bunch of seeds and pick the one that gives you the best performance.
:::


## Accessing the data 

```{r}
#| label: taxi-train-test
taxi_train <- training(taxi_split)
taxi_test <- testing(taxi_split)
```

### The training set 

```{r}
#| label: taxi-train
taxi_train
```

### The test set 

üôà

There are `r nrow(taxi_test)` rows and `r ncol(taxi_test)` columns in the test set.

## Your turn

![](img/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

* ### *Split your data so 20% is held out for the test set and show the first 10 rows of so of the training data*

```{r}
#| echo: false
#| results: false

set.seed(123)

taxi_split <- initial_split(taxi, prop = 0.8)
taxi_train <- training(taxi_split)
taxi_test <- testing(taxi_split)

nrow(taxi_train)
nrow(taxi_test)
```

* ### *Try out different values in `set.seed()` to see how the results change.*

## Exploratory data analysis

As you know by now, before we get too far into our data analysis, we want to do some exploration with visualizations. So lets get to it. 

## Your turn

![](img/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

* ### *What's the distribution of the outcome, `tip`?*
* ### *What's the distribution of numeric variables like `distance`?*
* ### *How does `tip` differ across the categorical variables?*

```{r}
#| include: false
#| fig-align: 'center'
taxi_train %>% 
  ggplot(aes(x = tip)) +
  geom_bar()
```

```{r}
#| include: false
#| fig-align: 'center'
taxi_train %>% 
  ggplot(aes(x = distance)) +
  geom_histogram(bins = 100) +
  facet_grid(vars(tip))
```

```{r}
#| include: false
#| fig-align: 'center'
taxi_train %>% 
  ggplot(aes(x = hour, fill = tip)) +
  geom_bar(position = "fill")
```

```{r}
#| include: false
#| fig-align: 'center'
taxi_train %>% 
  ggplot(aes(x = tip, fill = local)) +
  geom_bar() +
  scale_fill_viridis_d(end = .5)
```

::: {.callout-caution collapse="true"}
## Open only after you've attempted to generate a few plots on your own

For `taxi-tip-counts`, what about using `geom_bar()` with just `tip`?

For `taxi-tip-by-distance`, I would recommend `geom_histograms` and play around with the bin size. You will also want to incorporate `facet_grid`.

For `taxi-tip-by-hour`, I would plot making use of `fill=` and `geom_bar`

For `taxi-tip-by-local`, I'll leave this one up to you, but add a colore palette for color-blind people. 
:::

## Split smarter

Based on our EDA, we know that the source data contains fewer `"no"` tip values than `"yes"`. We want to make sure we allot equal proportions of those responses so that both the training and testing data have enough of each to give accurate estimates.

**Stratified sampling** would split within response values

::: {.callout-caution collapse="true"}
## Proof that there are fewer "no" tips through EDA
```{r}
#| label: taxi-tip-pct
#| echo: false
taxi %>%
  ggplot(aes(x = "", fill = tip)) +
  geom_bar(position = "fill") +
  labs(x = "")
```

:::

:::callout-tip
- For stratification involving regression, determine the quartiles of the data set and sample within those artificial groups
- For time series, we often use the most recent data as the test set
:::

## Your turn

![](img/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

* ### *Go back and modify your earlier code where you split the data to stratify by tip* 

```{r}
#| echo = FALSE
set.seed(123)

taxi_split <- initial_split(taxi, prop = 0.8, strata = tip)
taxi_train <- training(taxi_split)
taxi_test <- testing(taxi_split)
```

# PART 3: Fitting a model

`r hexes("parsnip")`

## Do it the tidymodels way 

-   Choose a model
-   Specify an engine
-   Set the mode

### Choose a model

The model type differentiates basic modeling approaches, such as linear regression, logistic regression, linear support vector machines, etc.

:::callout-note
## What models are available to you?

All available models are listed at <https://www.tidymodels.org/find/parsnip/> 
:::

How about we start with a good ol' linear regression model?

```{r logistic-reg}
linear_reg()
```

### Specify an engine

**Question:** How many ways are there to fit a linear model in R?

#### Lots of ways!

-   `lm` for linear model (default)

-   `glm` for generalized linear model

-   `glmnet` for regularized regression

-   `keras` for regression using TensorFlow

-   `stan` for Bayesian regression

-   `spark` for large data sets


The computational engine indicates how the model is fit, such as with a specific R package implementation or even methods outside of R like Keras or Stan. 

::: callout-note
## Note that some models when you call them have a default engine associated with them, but these can be easily overridden depending on your needs
:::

```{r logistic-reg-glmnet}
linear_reg() %>% 
  set_engine("glm", family = stats::poisson(link = "log"))

```

```{r logistic-reg-stan}
linear_reg() %>%
  set_engine("stan")
```

### Set the mode

The mode denotes in what kind of modeling context it will be used (most commonly, classification or regression)

::: callout-note
Note that some models have a default mode, some do not
:::

```{r decision-tree}
decision_tree()
```

```{r decision-tree-2}
decision_tree() %>% 
  set_mode("classification")
```

## Your turn

![](img/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

* ### *Copy the chunk below and edit the code to create a logistic regression model*

```{r}
#| eval: false

# Model
linear_reg()

# Engine
linear_reg() %>%
  set_engine("glmnet")

# Mode (some models have a default mode, others don't)
decision_tree() %>% 
  set_mode("regression")
```

## Models we'll be using today

* Logistic regression (you)
* Decision trees (me)

```{r sim-model-viz}
#| echo: false

set.seed(1)
dat <- sim_logistic(500, ~ .1 + 2 * A)
dat$bin <- cut(dat$A, breaks = c(seq(-3, 3, by = 1/2)), include.lowest = TRUE)
bin_midpoints <- data.frame(A = seq(-3, 3, by = 1/2) + 0.25)

rates <- 
  dat %>% 
  nest(.by = bin) %>% 
  mutate(
    probs = map(data, ~ binom.test(sum(.x$class == "one"), nrow(.x))),
    probs = map(probs, ~ tidy(.x))
  ) %>% 
  select(-data) %>% 
  unnest(cols = probs) %>% 
  arrange(bin) %>% 
  mutate(A = seq(-3, 3, by = 1/2) + 0.25) 

plot_rates <- left_join(rates, bin_midpoints, by = join_by(A)) %>% 
  filter(-2.5 < A, A < 3) %>% 
  ggplot() + 
  geom_point(aes(A, estimate)) +
  geom_errorbar(aes(A, estimate, ymin = conf.low, ymax = conf.high), width = .25)  +
  xlim(c(-3, 3.5)) +
  theme_bw(base_size = 18)
```

### Logistic regression

**The concept of odds and probability:**

- The **probability** of some event happening can be expressed as a value between 0 and 1
- If the probability of an event occurring is ${p}$, the **odds** of an event occurring can be expressed as $\frac{p}{1 - p}$ 

"If there is a 75% chance (or 0.75 probability) that it will rain today, then... the odds of it raining are 3 to 1, meaning that it is 3 times more likely to rain than not."

**From odds to log-odds:**

- It's challenging to work with odds in mathematical modeling, especially when there are multiple factors that might affect an outcome
- This is why we take the logarithm of the odds which puts it on a scale that is easier to work with

**The Logistic Regression Equation:**

$log(\frac{p}{1 - p}) = \beta_0 + \beta_1\cdot \text{A}$

- $log(\frac{p}{1 - p})$ is the log-odds (or the logit) of the probability of our event of interest (e.g., getting a tip)
- $\beta_0$ is the intercept, which represents the log-odds of the event when predictor A is zero
- $\beta_1\cdot \text{A}$ is the slope of coefficient for predictor `A`, indicating how much the log-odds of the event changes with a one-unit change in `A`

:::callout-note
It is common to convert the log-odds units into what is called an odds ratio (OR)
:::

**Relating this back to taxi's and tips**

- We want to predict whether someone will tip ("one") or not based on the trip distance, in odometer meters (`A`).
- $\beta_0$ would represent the log-odds of receiving a tip with zero distance
- $\beta_1$ shows how the log-odds of receiving a tip with each additional meter in distance
- Maybe as distance increases, the probability of getting a tip increases? (but not linearly- there are diminishing returns after a certain point)

### A possibly helpful visual

- Lets simulate a dataset of 500 observations with a single predictor `A` using our logistic regression equation. We ensure that the log-odds of the binary outcome (`class` with levels "zero" and "one") increases with respect to `A` with an intercept of `.1` and a slope of `2`. 
- Estimate the probability (converting log-odds to probabilities) of the "one" class for different values of `A` and plot these (`A` values are binned in 0.5 increments; error bars show the confidence intervals for estimates)
- The visual demonstrates the concept of the odds and probabilities changing in a non-linear fashion across the range of `A`, a key concept in logistic regression. 

What a logistic regression is doing is trying to fit this sort of sigmoid line to separate the two classes in an outcome. 

```{r plot-logistic-reg}
#| echo: false
#| fig.width: 6
#| fig.height: 5

logistic_preds <- logistic_reg() %>% 
  fit(class ~ A, data = dat) %>% 
  augment(new_data = bin_midpoints) 

plot_rates +
  geom_line(aes(A, .pred_one, color = I("#cd4173")), linewidth = 2, alpha = 0.8, data = logistic_preds) + 
  labs(y="Estimated Probability of 'One' Class", x="A")
```

### Decision trees

**An analogy**

- Let's play 20 questions: I'm thinking of a person, place, or thing. Guess it with asking "yes" or "no" questions

:::{.callout-caution collapse=true}
## Seriously, can you guess what or who or where I'm thinking about?

![](img/titantic.jpeg){width="200" fig-align="center"}
:::

**A slightly more formal characterization**

- A decision tree is a flowchart-like tree structure consisting of splits or if/then statements based on predictors

**But to explain, visualizations are best for decision trees**

```{r}
#| echo: false
library(titanic)
library(tidymodels)
library(parsnip)

# Load the Titanic training data
data("titanic_train")

# Clean the data: remove rows with missing target, select features
titanic_clean <- titanic_train %>%
  filter(!is.na(Survived)) %>%
  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>%
  mutate(Survived = as.factor(Survived), # Ensure Survived is a factor
         Sex = as.factor(Sex),
         Embarked = as.factor(Embarked),
         Pclass = as.factor(Pclass))

# Split the data into training and testing sets
set.seed(123)
titanic_split <- initial_split(titanic_clean, prop = 0.75)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)

# Define the model using the parsnip package
tree_model <- decision_tree(mode = "classification") %>%
  set_engine("rpart") %>%
  set_mode("classification")

# Fit the model to the training data
tree_fit <- tree_model %>%
  fit(Survived ~ ., data = titanic_train)

# Visualize the tree
rpart.plot::rpart.plot(tree_fit$fit, roundint = FALSE)
```

:::{.callout-caution collapse=true}
## Variable explanations

`0` corresponds to "Did Not Survive"; `1` corresponds to "Survived"

`Pclass:` Passenger Class. This is a categorical feature indicating the class of travel of the passenger aboard the Titanic. It typically has three categories:
- 1 for first-class (the highest class),
- 2 for second-class,
- 3 for third-class (the lowest class).

`Sex:` The sex of the passenger (male or female). This is a binary categorical feature.

`Age:` The age of the passenger. This is a continuous numerical feature.

`SibSp:` The number of siblings or spouses the passenger had aboard the Titanic. This is a discrete numerical feature.

`Parch:` The number of parents or children the passenger had aboard the Titanic. This is also a discrete numerical feature. Note that some relations, like nannies or friends, may not be included in this count.

`Fare:` The amount of money the passenger paid for the ticket. This is a continuous numerical feature.
:::

**A few things to keep in mind**

- A tree chooses a feature and a split point (e.g., if `Age >= 6.5`) to partition the data into subsets that are as homogeneous "pure" as possible (meaning that the node contains data points from predominantly one class)
  - Just know there are measures for evaluating the "purity" of a node, e.g., Gini Impurity, Entropy, Classification Error
- This process is repeated recursively, forming a tree structure until certain stopping criteria are met (e.g., when a node has a small number of observations, some maximum depth).
- Trees are also **pruned** to reduce its complexity

### Another possibly helpful visual 

```{r tree-fit}
#| echo: false
#| fig.width: 6
#| fig.height: 5

tree_fit <- decision_tree(mode = "classification") %>% 
  fit(class ~ A, data = mutate(dat, class = forcats::fct_rev(class)))

tree_preds <- augment(tree_fit, new_data = bin_midpoints)
```

```{r plot-tree-fit}
#| echo: false
#| fig.width: 5
#| fig.height: 4.5
#| fig-align: center

library(rpart.plot)
tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE) 
```

```{r plot-tree-preds}
#| echo: false
#| fig.width: 6
#| fig.height: 5

plot_rates +
  geom_step(aes(A, .pred_one, col = I("#cd4173")), linewidth = 2, alpha = 0.8, data = tree_preds) +
  labs(y="Estimated Probability of 'One' Class", x="A")
```

`r hexes("workflows")`

## Model workflows 

- In tidymodels, there is this idea that a model-oriented data analysis consists of
a preprocessor and a model, and these should be built into a single "workflow" object
  - **Preprocessors** range from simple formulas `y ~ x1 + x2` to sophisticated `recipes` that allow feature engineering operations (e.g., extract features using a PCA), remove variables with zero variances, categorizes missing categorical data (NA's) as `unknown`, dummy codes categorical variables, and lots more
  - **Models** allow different ways to fit the data  

- **By packaging preprocessing steps and modeling in one workflow, lots of advantages!**
  - Ensures that new data goes through the same preprocessing steps as trianing data
  - Ensures consistency and reproducibility
  - They can help organize your work when working with multiple models
  - Handles new data better than base R tools in terms of new factor levels

`r hexes("parsnip", "workflows")`

## A model workflow 

```{r tree-wflow}

#logi_spec <-
tree_spec <-                            # set up all the specs on model
  decision_tree(cost_complexity = 0.002) %>%   
  set_mode("classification")

#logi_wflow <-
tree_wflow <- workflow() %>%            # build the workflow
  add_formula(tip ~ .) %>%              # preprocessor steps (could be lots!)
  add_model(tree_spec)                  # add model

#logi_fit <-                            
tree_fit <-                             # now fit to training data 
  tree_wflow %>% 
  fit(data = taxi_train) 
```

::: callout-tip
"Shortcut" by specifying the preprocessor and model specs directly in the `workflow()` call:

```r
workflow(tip ~ ., tree_spec) %>% 
  fit(data = taxi_train) 
```  
:::

## Your turn

![](img/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

* ### *Copy the chunk above "{r tree-wflow}" and edit the code to create a workflow for a logistic regression model*

  -   Make sure you swap out `tree_*` for `logi_*` when saving each set of code 

`r hexes("recipes")`

## Pre-processing: Adding "recipes" 

  -   Hypothetical additions just for illustrative purposes 

```{r}
#| eval: false
tree_rec <-
  step_date(some_var1, features = c("dow", "month", "year")) %>% 
  step_rm(some_var1) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  step_pca(all_numeric_predictors())

#logi_wflow <-
tree_wflow <- workflow() %>%            # build the workflow
  add_formula(tip ~ .) %>%              # preprocessor: add formula
  add_recipe(tree_rec) %>%              # preprocessor: add recipe
  add_model(tree_spec)                  # add model
```

  -   Best to watch the lectures for more detail. 
  -   You can also visit <https://www.tidymodels.org/find/recipes/> for details on all possible recipes  

## How do you **understand** your new `tree_fit` model?

<!-- Could talk about broom methods for looking at fitted objects: glance, tidy, only focusing on augment as set up for prediction -->

#### Predict with your model

::: {.callout-caution collapse=true}
## What happens when you run `predict(logi_fit, new_data = taxi_test)`

```{r}
predict(tree_fit, new_data = taxi_test)
```
:::

```{r}
augment(tree_fit, new_data = taxi_test)
```


:::callout-note
## The tidymodels prediction guarantee!
-   The predictions will always be inside a **tibble** 
-   The column names and types are **unsurprising** and **predictable**
-   The number of rows in `new_data` and the output **are the same**

:::

#### Bonus: For decision trees, visualization

```{r plot-tree-fit-4}
#| echo: false
#| fig-align: center
library(rpart.plot)
tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)
# `roundint = FALSE` is only to quiet a warning
```

:::callout-note
## Additional methods for understanding models

- Overall variable importance, such as with the [vip](https://koalaverse.github.io/vip/) package 

- Flexible model explainers, such as with the [DALEXtra](https://dalex.drwhy.ai/) package. *Works by removing a variable and see the effect on prediction, if the variable is important it will really wreck with the fit accuracy* 
:::

Learn more at <https://www.tmwr.org/explain.html>

## Your turn

![](img/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

* ### *For the fitted logistic regression model, use `augment`, then `glance` and `tidy`. What happens with each?*

`r hexes("yardstick")` 

# PART 4: Metrics for model performance 

::: callout-warning
In the following we will be working on evaluation metrics based on the training set data, but keep in mind that you also need to run the metrics on the test data. If you fail to do so, it can give a misleading impression of model performance. Hopefully this will be clearer soon.  
:::

```{r taxi-fit-augment}
augment(tree_fit, new_data = taxi_train) %>%
  relocate(tip, .pred_class, .pred_yes, .pred_no)
```

## Confusion matrix

![](img/confusion-matrix.png)



`conf_mat()` can be used to see how well the model is doing at prediction

```{r conf-mat-plot}
augment(tree_fit, new_data = taxi_train) %>%
  conf_mat(truth = tip, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

## Accuracy

This is a straightforward measure of the proportion of correct predictions out of all predictions. 

![](img/confusion-matrix-accuracy.png){width=500}

```{r acc-2}
augment(tree_fit, new_data = taxi_train) %>%
  mutate(.pred_class = factor("yes", levels = c("yes", "no"))) %>%
  accuracy(truth = tip, estimate = .pred_class)
```

:::callout-warning
## Dangers of accuracy

We need to be careful of using `accuracy()` since it can give "good" performance by only predicting one way with imbalanced data
:::

## Sensitivity

Where accuracy provides a quick understanding of how often the model is correct, sensitivity is an important measure when the cost of missing a positive case (false negative) is high. In our scenario, a positive case is the outcome variable coded as "1" (tip: yes). In other scenarios, this is more consequential (such as correctly predicting the percentage of sick people who have some targeted condition).

![](img/confusion-matrix-sensitivity.png){width=500}

```{r sens}
augment(tree_fit, new_data = taxi_train) %>%
  sensitivity(truth = tip, estimate = .pred_class)
```

## Specificity

This is essentially the compliment of sensitivity. Specificity is an important measure of how well the model correctly predicts the true negative. In our scenario, a true negative is the outcome variable coded as "0" (tip: no). Pay attention to specificity when the cost of a false positive is high (like convicting an innocent person).

![](img/confusion-matrix-specificity.png){width=500}

```{r spec}
augment(tree_fit, new_data = taxi_train) %>%
  specificity(truth = tip, estimate = .pred_class)
```

## Combining the metrics

We can use `metric_set()` to combine multiple calculations into a single table

```{r taxi-metrics}
taxi_metrics <- metric_set(accuracy, specificity, sensitivity)

augment(tree_fit, new_data = taxi_train) %>%
  taxi_metrics(truth = tip, estimate = .pred_class)
```

All yardstick metric functions work with grouped data frames!

```{r taxi-metrics-grouped}
taxi_metrics <- metric_set(accuracy, specificity, sensitivity)

augment(tree_fit, new_data = taxi_train) %>%
  group_by(local) %>%
  taxi_metrics(truth = tip, estimate = .pred_class)
```

## Your turn

![](img/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

* ## *Compute a confusion matrix for your logistic model and produce a table that shows its accuracy, specificity, sensitivity scores*

:::callout-note
## An important aside: Two class data

These metrics assume that we know the threshold for converting "soft" probability predictions into "hard" class predictions.

Is a 50% threshold good? 

What happens if we say that we need to be 80% sure to declare an event?

-   sensitivity ‚¨áÔ∏è, specificity ‚¨ÜÔ∏è  In other words: more likely to predict negatives(no tips)

What happens for a 20% threshold? 

-   sensitivity ‚¨ÜÔ∏è, specificity ‚¨áÔ∏è In other words: more likely to predict positives(yes tips)

```{r}
#| label: thresholds
#| echo: false

augment(tree_fit, new_data = taxi_train) %>% 
  roc_curve(truth = tip, .pred_yes) %>% 
  filter(is.finite(.threshold)) %>% 
  pivot_longer(c(specificity, sensitivity), names_to = "statistic", values_to = "value") %>% 
  rename(`event threshold` = .threshold) %>% 
  ggplot(aes(x = `event threshold`, y = value, col = statistic, group = statistic)) + 
  geom_line() +
  scale_color_brewer(palette = "Dark2") +
  labs(y = NULL) +
  coord_equal() +
  theme(legend.position = "top")
```

:::

`r hexes("yardstick")`

## ROC curves 

An ROC (receiver operator characteristic) curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system. 

**It primarily:**

- Calculates the sensitivity and specificity for all possible thresholds. It helps you (in a visual way) understand whether a model is optimally balancing sensitivity and specificity in a way that makes sense

**It is created by:**

- Plotting true positive rate (TPR, or sensitivity) against the false positive rate (FPR, 1-sensitivity) at various threshold settings
- In other words: plotting the rate in which the models correctly predict "tip=yes" and the rate in which the models mistakenly predict that "tip=yes" at various points above which a prediction is considered to be of the positive class. 

:::callout-note
## Why 1-sensitivity for FPR?

Given that sensitivity is the true positive rate, and specificity is the true negative rate. Hence `1 - specificity` is the false positive rate.
:::

**Here's the ROC curve for our model based on the training data:**

```{r roc-curve}
#| fig-width: 6
#| fig-height: 6
#| output-location: "column"

augment(tree_fit, new_data = taxi_train) %>% 
  roc_curve(truth = tip, .pred_yes) %>%
  autoplot() +
  labs(x="FPR, 1-specificity", y="TPR, or sensitivity")
```
**How do we interpret:**

- A high Area Under the Curve (AUC) indicates that for the majority of thresholds, the TPR (sensitivity) was high and the FPR was low. It means that the model has a good measure of separability and is capable of distinguishing between the positive and negative classes across a wide range of thresholds.

- ROC AUC = 1 üíØ 
- ROC AUC = 1/2 üò¢

**Let's get the actual value for AUC:** 

```{r roc-auc}
augment(tree_fit, new_data = taxi_train) %>% 
  roc_auc(truth = tip, .pred_yes)
```

::: callout-note
ROC curves are insensitive to class imbalance.
:::

::: callout-note
When you look at a ROC curve, each point corresponds to a threshold, but the curve itself does not tell you what these thresholds are. To find out, you would need to refer back to the data used to create the ROC curve.
:::


## Your turn

![](img/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

* ### *Compute and plot an ROC curve for your logistic model*


# PART 5: Using resampling to estimate performance

## ‚ö†Ô∏è Dangers of Overfitting ‚ö†Ô∏è

Models can easily be overfitted to the training data, meaning they perform well on the training data but poorly on unseen data. 

### Fitting to your training data

![](https://raw.githubusercontent.com/topepo/2022-nyr-workshop/main/images/tuning-overfitting-train-1.svg)

### Problems when generalizing to test data

![](https://raw.githubusercontent.com/topepo/2022-nyr-workshop/main/images/tuning-overfitting-test-1.svg)

### Is our model doing well on training but poorly on new data? 

<!-- We call this "resubstitution" or "repredicting the training set" -->
<!-- We call this a "resubstitution estimate" -->

**Here's our model performance on the training data**

```{r augment-train}
tree_fit %>%
  augment(taxi_train)
```

```{r augment-acc}
tree_fit %>%
  augment(taxi_train) %>%
  accuracy(tip, .pred_class)
```

**Here's our model performance on the test data (unseen data)**

```{r augment-acc-test}
tree_fit %>%
  augment(taxi_test) %>%
  accuracy(tip, .pred_class)
``` 


### Better to fit on training data via a process of resampling

Resampling is basically an empirical simulation system used to understand how well the model would work on new data. It's a more robust estimate of the model's performance 

## V-fold (K-fold) Cross-validation

- We want to repeatedly sample ("fold") the training data to create unique subsets (e.g., Resample 1, Resample 2, Resample *B*) for analysis and assessment  

![](https://www.tmwr.org/premade/resampling.svg)

- Here, we randomly split the training data into V (or K) distinct blocks of roughly equal size (AKA the "folds")

![](https://www.tmwr.org/premade/three-CV.svg){fig-align="center"}

- In Fold 1:
  - We leave out the first block of analysis data and fit a model
  - This model is used to predict the held-out block of assessment data
  
- We continue this process until we've predicted all V assessment blocks
- This process ensures that every observation from the original dataset has the chance of appearing in the analysis and assessment set

![](https://www.tmwr.org/premade/three-CV-iter.svg)

:::callout-note
## Other advantages of resampling with V-fold cross-validation

- **Efficient use of data:** When the amount of data is limited, it's a way to use every data point in both the training and validation

- **Model tuning:** Useful method for selecting the model with the best tuning parameters (hyperparameters). You can perform cross-validation for various combinations of parameters and choose the one that performs best. 

- **Comparing models** What if we want to compare more models to see if there are important differences? Cross-validation allows us to do just this with just the *training* data
:::

## Your turn

![](img/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

* ### *If we use 10 folds, what percent of the training data ends up in ANALYSIS for each fold? for ASSESSMENT for each fold?*

`r hexes("rsample")`

## Let's do some cross-validation 

```{r vfold-cv}
vfold_cv(taxi_train) # v = 10 is default
```

**What is in this?**

```{r taxi-splits}
taxi_folds <- vfold_cv(taxi_train)
taxi_folds$splits[1:3]
```

::: callout-note
These separate sets are in a list column, which is a way to store non-atomic types in a dataframe
:::

**We can do this with how many folds we want**

```{r vfold-cv-v}
vfold_cv(taxi_train, v = 5)
```

**Stratification often helps, with very little downside**

```{r vfold-cv-strata}
vfold_cv(taxi_train, strata = tip)
```

### We'll use this setup:

:::callout-important
Set the seed when creating resamples
:::

```{r taxi-folds}
set.seed(123)
taxi_folds <- vfold_cv(taxi_train, v = 10, strata = tip)
taxi_folds
```

## Fit our model to the resamples!

We will fit 10 models on 10 slightly different analysis sets.

```{r fit-resamples}
taxi_res <- fit_resamples(tree_wflow, taxi_folds)
taxi_res
```

```{r}
#| include: false
ctrl <- control_resamples(save_pred = TRUE)
taxi_res <- fit_resamples(tree_wflow, taxi_folds, control = ctrl)
taxi_res
```

```{r}
#| include: false
taxi_pred <- collect_predictions(taxi_res)
taxi_pred %>% slice(1:4)
```

```{r}
#| include: false
taxi_pred %>% 
  group_by(id) %>%
  taxi_metrics(truth = tip, estimate = .pred_class)
```

:::callout-tip
## Alternate resampling schemes

`r hexes("rsample")`

## Bootstrapping

- Assess model accuracy and stability

- From your original training dataset, create many (often thousands) of new "bootstrap" training datasets by sampling with replacement. 

![](https://www.tmwr.org/premade/bootstraps.svg)

- After running the bootstrap process many times, you'll end up with a distribution of performance metrics (like accuracy or AUC).

**Handling Small Datasets:** It is particularly useful when dealing with small datasets where you might not be able to afford to set aside a portion of the data as a test set.

**Uncertainty Estimates:** It provides a way to create confidence intervals for various performance metrics, giving a sense of how uncertain those metrics are.


```{r bootstraps}
set.seed(3214)
bootstraps(taxi_train)
```

<https://rsample.tidymodels.org/reference/index.html>

:::


## Your turn

![](img/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

* ### *Now run `vfold_cv` to get 10 splits and refit your logistic regression workflow (using `fit_resamples`)*

`r hexes("tune")`

## Evaluating model performance 

- The goal of resampling is to produce a single estimate of performance for a model
- The final performance is based on the hold-out predictions by averaging the statistics from the V folds.

:::callout-note
Even though we end up estimating V models, these models are discarded after we have our performance estimate. üóë
:::

```{r collect-metrics}
taxi_res %>%
  collect_metrics()
```

### We can reliably measure performance using only the **training** data üéâ

::: callout-note
`collect_metrics()` is one of a suite of `collect_*()` functions that can be used to work with columns of tuning results. Most columns in a tuning result prefixed with `.` have a corresponding `collect_*()` function with options for common summaries.
:::

<!--```{r}
library(shinymodels)
explore(taxi_res, 
        hover_cols = c(tip))
```
-->

## Your turn

![](img/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

* ### *Use `collect_metrics()` to get a single estimates of performance on your cross-validated logistic regression*

* ### *Generate a new confusion matrix for your cross-validated logistic regression*

### Does our model perfomance change when fit once on the training data vs resampling on the training data?

**Do you remember this from way back when? This is the ROC AUC from no resampling:**

```{r roc-auc-2}
augment(tree_fit, new_data = taxi_train) %>% 
  roc_auc(truth = tip, .pred_yes)
```

**Now, this is from resampling:**

```{r}
taxi_res %>%
  collect_metrics() %>% 
  select(.metric, mean, n)
```
#### Lesson is that the fitting a model on just the training set (without resampling) gives you overly optimistic metrics ‚ö†Ô∏è  

## What you would report

- **Training Performance (Cross-Validation):**
  - Present metrics that summarize the model's performance across the folds
    - mean and standard deviation of accuracy
    - AUC
    - sensitivity
    - specificity
    
This gives an indication of how well the model is expected to perform on unseen data, taking into account the variability due to different subsets of the training data.

- **Test Set Performance:**

```{r}
taxi_metrics <- metric_set(accuracy, specificity, sensitivity)

augment(tree_fit, new_data = taxi_test) %>%
  taxi_metrics(truth = tip, estimate = .pred_class)
```

```{r}
augment(tree_fit, new_data = taxi_test) %>% 
  roc_auc(truth = tip, .pred_yes)
```

```{r}
#| include: false
# might be best for logistic regression
# test_results <- tree_fit %>%
#   predict(taxi_test) %>%
#   bind_cols(taxi_test) %>%
#   metrics(truth = tip, estimate = .pred_class)
# test_results
```


The performance metrics derived from the held-out test set give the reader an understanding of how the model performs on completely unseen data. This is the ultimate test of the model's generalizability and is often considered the most important set of metrics to report.

- **Overfitting Discussion**

Discuss any signs of overfitting observed (for instance, if there was a large discrepancy between the cross-validation performance and the test set performance).

- **Model Interpretability** 

It's often important to discuss what the model's parameters or predictions mean in the context of the subject area.

## Your turn

![](img/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

* ### *Generate a final report for your regression logistic model, but now using the test set*

## Completing the whole game

![](img/whole-game-final-performance.jpg)

# PART 6: Next Steps?

## Random forest üå≥üå≤üå¥üåµüå≥üå≥üå¥üå≤üåµüå¥üå≥üåµ

- Ensemble many decision tree models
  - All the trees vote! üó≥Ô∏è
  - Bootstrap aggregating + random predictor sampling

##  Model tuning 

- Requires tuning model parameters with more advanced machine learning methods
  - Growing phase; tree pruning
    - Grid search
    - Iterative search
  - Nice resource: <https://topepo.github.io/2022-nyr-workshop/5-tuning.html#1>
  
##  Variable importance

- `vip` package

