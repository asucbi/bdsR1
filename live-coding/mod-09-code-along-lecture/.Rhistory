labs(x = NULL, y = NULL)
all_split
set.seed(123)
taxi_split <- initial_split(taxi)
taxi_split
#| label: taxi-train-test
taxi_train <- training(taxi_split)
taxi_test <- testing(taxi_split)
#| label: taxi-train
taxi_train
#| echo: false
#| results: false
set.seed(123)
taxi_split <- initial_split(taxi, prop = 0.8)
taxi_train <- training(taxi_split)
taxi_test <- testing(taxi_split)
nrow(taxi_train)
nrow(taxi_test)
#| include: false
#| fig-align: 'center'
taxi_train %>%
ggplot(aes(x = tip)) +
geom_bar()
#| include: false
#| fig-align: 'center'
taxi_train %>%
ggplot(aes(x = distance)) +
geom_histogram(bins = 100) +
facet_grid(vars(tip))
#| include: false
#| fig-align: 'center'
taxi_train %>%
ggplot(aes(x = hour, fill = tip)) +
geom_bar(position = "fill")
#| include: false
#| fig-align: 'center'
taxi_train %>%
ggplot(aes(x = tip, fill = local)) +
geom_bar() +
scale_fill_viridis_d(end = .5)
#| label: taxi-tip-pct
#| echo: false
taxi %>%
ggplot(aes(x = "", fill = tip)) +
geom_bar(position = "fill") +
labs(x = "")
#| echo = FALSE
set.seed(123)
taxi_split <- initial_split(taxi, prop = 0.8, strata = tip)
taxi_train <- training(taxi_split)
taxi_test <- testing(taxi_split)
linear_reg()
linear_reg() %>%
set_engine("glm", family = stats::poisson(link = "log"))
linear_reg() %>%
set_engine("stan")
decision_tree()
decision_tree() %>%
set_mode("classification")
#| echo: false
set.seed(1)
dat <- sim_logistic(500, ~ .1 + 2 * A)
dat$bin <- cut(dat$A, breaks = c(seq(-3, 3, by = 1/2)), include.lowest = TRUE)
bin_midpoints <- data.frame(A = seq(-3, 3, by = 1/2) + 0.25)
rates <-
dat %>%
nest(.by = bin) %>%
mutate(
probs = map(data, ~ binom.test(sum(.x$class == "one"), nrow(.x))),
probs = map(probs, ~ tidy(.x))
) %>%
select(-data) %>%
unnest(cols = probs) %>%
arrange(bin) %>%
mutate(A = seq(-3, 3, by = 1/2) + 0.25)
plot_rates <- left_join(rates, bin_midpoints, by = join_by(A)) %>%
filter(-2.5 < A, A < 3) %>%
ggplot() +
geom_point(aes(A, estimate)) +
geom_errorbar(aes(A, estimate, ymin = conf.low, ymax = conf.high), width = .25)  +
xlim(c(-3, 3.5)) +
theme_bw(base_size = 18)
#| echo: false
#| fig.width: 6
#| fig.height: 5
logistic_preds <- logistic_reg() %>%
fit(class ~ A, data = dat) %>%
augment(new_data = bin_midpoints)
plot_rates +
geom_line(aes(A, .pred_one, color = I("#cd4173")), linewidth = 2, alpha = 0.8, data = logistic_preds) +
labs(y="Estimated Probability of 'One' Class", x="A")
#| echo: false
library(titanic)
library(tidymodels)
library(parsnip)
# Load the Titanic training data
data("titanic_train")
# Clean the data: remove rows with missing target, select features
titanic_clean <- titanic_train %>%
filter(!is.na(Survived)) %>%
select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>%
mutate(Survived = as.factor(Survived), # Ensure Survived is a factor
Sex = as.factor(Sex),
Embarked = as.factor(Embarked),
Pclass = as.factor(Pclass))
# Split the data into training and testing sets
set.seed(123)
titanic_split <- initial_split(titanic_clean, prop = 0.75)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
# Define the model using the parsnip package
tree_model <- decision_tree(mode = "classification") %>%
set_engine("rpart") %>%
set_mode("classification")
# Fit the model to the training data
tree_fit <- tree_model %>%
fit(Survived ~ ., data = titanic_train)
# Visualize the tree
rpart.plot::rpart.plot(tree_fit$fit, roundint = FALSE)
#| echo: false
#| fig.width: 6
#| fig.height: 5
tree_fit <- decision_tree(mode = "classification") %>%
fit(class ~ A, data = mutate(dat, class = forcats::fct_rev(class)))
tree_preds <- augment(tree_fit, new_data = bin_midpoints)
#| echo: false
#| fig.width: 5
#| fig.height: 4.5
#| fig-align: center
library(rpart.plot)
tree_fit %>%
extract_fit_engine() %>%
rpart.plot(roundint = FALSE)
#| echo: false
#| fig.width: 6
#| fig.height: 5
plot_rates +
geom_step(aes(A, .pred_one, col = I("#cd4173")), linewidth = 2, alpha = 0.8, data = tree_preds) +
labs(y="Estimated Probability of 'One' Class", x="A")
#logi_spec <-
tree_spec <-                            # set up all the specs on model
decision_tree(cost_complexity = 0.002) %>%
set_mode("classification")
#logi_wflow <-
tree_wflow <- workflow() %>%            # build the workflow
add_formula(tip ~ .) %>%              # preprocessor steps (could be lots!)
add_model(tree_spec)                  # add model
#logi_fit <-
tree_fit <-                             # now fit to training data
tree_wflow %>%
fit(data = taxi_train)
predict(tree_fit, new_data = taxi_test)
augment(tree_fit, new_data = taxi_test)
#| echo: false
#| fig-align: center
library(rpart.plot)
tree_fit %>%
extract_fit_engine() %>%
rpart.plot(roundint = FALSE)
# `roundint = FALSE` is only to quiet a warning
tree_fit %>%
extract_fit_engine()
library(tidymodels)
glimpse(taxi)
#| include: false
#| file: setup.R
#| message: true
library(tidymodels)
library(tidymodels)
glimpse(taxi)
#| echo: false
#| fig.width: 12
#| fig.height: 3
set.seed(123)
library(forcats)
one_split <- slice(taxi, 1:30) %>%
initial_split() %>%
tidy() %>%
add_row(Row = 1:30, Data = "Original") %>%
mutate(Data = case_when(
Data == "Analysis" ~ "Training",
Data == "Assessment" ~ "Testing",
TRUE ~ Data
)) %>%
mutate(Data = factor(Data, levels = c("Original", "Training", "Testing")))
all_split <-
ggplot(one_split, aes(x = Row, y = fct_rev(Data), fill = Data)) +
geom_tile(color = "white",
linewidth = 1) +
# scale_fill_manual(values = splits_pal, guide = "none") +
theme_minimal() +
theme(axis.text.y = element_text(size = rel(2)),
axis.text.x = element_blank(),
legend.position = "top",
panel.grid = element_blank()) +
coord_equal(ratio = 1) +
labs(x = NULL, y = NULL)
all_split
set.seed(123)
taxi_split <- initial_split(taxi)
taxi_split
#| label: taxi-train-test
taxi_train <- training(taxi_split)
taxi_test <- testing(taxi_split)
#| label: taxi-train
taxi_train
#| echo: false
#| results: false
set.seed(123)
taxi_split <- initial_split(taxi, prop = 0.8)
taxi_train <- training(taxi_split)
taxi_test <- testing(taxi_split)
nrow(taxi_train)
nrow(taxi_test)
#| include: false
#| fig-align: 'center'
taxi_train %>%
ggplot(aes(x = tip)) +
geom_bar()
#| include: false
#| fig-align: 'center'
taxi_train %>%
ggplot(aes(x = distance)) +
geom_histogram(bins = 100) +
facet_grid(vars(tip))
#| include: false
#| fig-align: 'center'
taxi_train %>%
ggplot(aes(x = hour, fill = tip)) +
geom_bar(position = "fill")
#| include: false
#| fig-align: 'center'
taxi_train %>%
ggplot(aes(x = tip, fill = local)) +
geom_bar() +
scale_fill_viridis_d(end = .5)
#| label: taxi-tip-pct
#| echo: false
taxi %>%
ggplot(aes(x = "", fill = tip)) +
geom_bar(position = "fill") +
labs(x = "")
#| echo = FALSE
set.seed(123)
taxi_split <- initial_split(taxi, prop = 0.8, strata = tip)
taxi_train <- training(taxi_split)
taxi_test <- testing(taxi_split)
linear_reg()
linear_reg() %>%
set_engine("glm", family = stats::poisson(link = "log"))
linear_reg() %>%
set_engine("stan")
decision_tree()
decision_tree() %>%
set_mode("classification")
#| echo: false
set.seed(1)
dat <- sim_logistic(500, ~ .1 + 2 * A)
dat$bin <- cut(dat$A, breaks = c(seq(-3, 3, by = 1/2)), include.lowest = TRUE)
bin_midpoints <- data.frame(A = seq(-3, 3, by = 1/2) + 0.25)
rates <-
dat %>%
nest(.by = bin) %>%
mutate(
probs = map(data, ~ binom.test(sum(.x$class == "one"), nrow(.x))),
probs = map(probs, ~ tidy(.x))
) %>%
select(-data) %>%
unnest(cols = probs) %>%
arrange(bin) %>%
mutate(A = seq(-3, 3, by = 1/2) + 0.25)
plot_rates <- left_join(rates, bin_midpoints, by = join_by(A)) %>%
filter(-2.5 < A, A < 3) %>%
ggplot() +
geom_point(aes(A, estimate)) +
geom_errorbar(aes(A, estimate, ymin = conf.low, ymax = conf.high), width = .25)  +
xlim(c(-3, 3.5)) +
theme_bw(base_size = 18)
#| echo: false
#| fig.width: 6
#| fig.height: 5
logistic_preds <- logistic_reg() %>%
fit(class ~ A, data = dat) %>%
augment(new_data = bin_midpoints)
plot_rates +
geom_line(aes(A, .pred_one, color = I("#cd4173")), linewidth = 2, alpha = 0.8, data = logistic_preds) +
labs(y="Estimated Probability of 'One' Class", x="A")
#| echo: false
library(titanic)
library(tidymodels)
library(parsnip)
# Load the Titanic training data
data("titanic_train")
# Clean the data: remove rows with missing target, select features
titanic_clean <- titanic_train %>%
filter(!is.na(Survived)) %>%
select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>%
mutate(Survived = as.factor(Survived), # Ensure Survived is a factor
Sex = as.factor(Sex),
Embarked = as.factor(Embarked),
Pclass = as.factor(Pclass))
# Split the data into training and testing sets
set.seed(123)
titanic_split <- initial_split(titanic_clean, prop = 0.75)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
# Define the model using the parsnip package
tree_model <- decision_tree(mode = "classification") %>%
set_engine("rpart") %>%
set_mode("classification")
# Fit the model to the training data
tree_fit <- tree_model %>%
fit(Survived ~ ., data = titanic_train)
# Visualize the tree
rpart.plot::rpart.plot(tree_fit$fit, roundint = FALSE)
#| echo: false
#| fig.width: 6
#| fig.height: 5
tree_fit <- decision_tree(mode = "classification") %>%
fit(class ~ A, data = mutate(dat, class = forcats::fct_rev(class)))
tree_preds <- augment(tree_fit, new_data = bin_midpoints)
#| echo: false
#| fig.width: 5
#| fig.height: 4.5
#| fig-align: center
library(rpart.plot)
tree_fit %>%
extract_fit_engine() %>%
rpart.plot(roundint = FALSE)
#| echo: false
#| fig.width: 6
#| fig.height: 5
plot_rates +
geom_step(aes(A, .pred_one, col = I("#cd4173")), linewidth = 2, alpha = 0.8, data = tree_preds) +
labs(y="Estimated Probability of 'One' Class", x="A")
#logi_spec <-
tree_spec <-                            # set up all the specs on model
decision_tree(cost_complexity = 0.002) %>%
set_mode("classification")
#logi_wflow <-
tree_wflow <- workflow() %>%            # build the workflow
add_formula(tip ~ .) %>%              # preprocessor steps (could be lots!)
add_model(tree_spec)                  # add model
#logi_fit <-
tree_fit <-                             # now fit to training data
tree_wflow %>%
fit(data = taxi_train)
predict(tree_fit, new_data = taxi_test)
augment(tree_fit, new_data = taxi_test)
#| echo: false
#| fig-align: center
library(rpart.plot)
tree_fit %>%
extract_fit_engine() %>%
rpart.plot(roundint = FALSE)
# `roundint = FALSE` is only to quiet a warning
augment(tree_fit, new_data = taxi_train) %>%
relocate(tip, .pred_class, .pred_yes, .pred_no)
augment(tree_fit, new_data = taxi_train) %>%
conf_mat(truth = tip, estimate = .pred_class) %>%
autoplot(type = "heatmap")
augment(tree_fit, new_data = taxi_train) %>%
mutate(.pred_class = factor("yes", levels = c("yes", "no"))) %>%
accuracy(truth = tip, estimate = .pred_class)
augment(tree_fit, new_data = taxi_train) %>%
sensitivity(truth = tip, estimate = .pred_class)
augment(tree_fit, new_data = taxi_train) %>%
specificity(truth = tip, estimate = .pred_class)
taxi_metrics <- metric_set(accuracy, specificity, sensitivity)
augment(tree_fit, new_data = taxi_train) %>%
taxi_metrics(truth = tip, estimate = .pred_class)
taxi_metrics <- metric_set(accuracy, specificity, sensitivity)
augment(tree_fit, new_data = taxi_train) %>%
group_by(local) %>%
taxi_metrics(truth = tip, estimate = .pred_class)
#| label: thresholds
#| echo: false
augment(tree_fit, new_data = taxi_train) %>%
roc_curve(truth = tip, .pred_yes) %>%
filter(is.finite(.threshold)) %>%
pivot_longer(c(specificity, sensitivity), names_to = "statistic", values_to = "value") %>%
rename(`event threshold` = .threshold) %>%
ggplot(aes(x = `event threshold`, y = value, col = statistic, group = statistic)) +
geom_line() +
scale_color_brewer(palette = "Dark2") +
labs(y = NULL) +
coord_equal() +
theme(legend.position = "top")
#| fig-width: 6
#| fig-height: 6
#| output-location: "column"
augment(tree_fit, new_data = taxi_train) %>%
roc_curve(truth = tip, .pred_yes) %>%
autoplot() +
labs(x="FPR, 1-specificity", y="TPR, or sensitivity")
augment(tree_fit, new_data = taxi_train) %>%
roc_auc(truth = tip, .pred_yes)
tree_fit %>%
augment(taxi_train)
tree_fit %>%
augment(taxi_train) %>%
accuracy(tip, .pred_class)
tree_fit %>%
augment(taxi_test) %>%
accuracy(tip, .pred_class)
vfold_cv(taxi_train) # v = 10 is default
taxi_folds <- vfold_cv(taxi_train)
taxi_folds$splits[1:3]
vfold_cv(taxi_train, v = 5)
vfold_cv(taxi_train, strata = tip)
set.seed(123)
taxi_folds <- vfold_cv(taxi_train, v = 10, strata = tip)
taxi_folds
taxi_res <- fit_resamples(tree_wflow, taxi_folds)
taxi_res
#| include: false
ctrl <- control_resamples(save_pred = TRUE)
taxi_res <- fit_resamples(tree_wflow, taxi_folds, control = ctrl)
taxi_res
#| include: false
taxi_pred <- collect_predictions(taxi_res)
taxi_pred %>% slice(1:4)
#| include: false
taxi_pred %>%
group_by(id) %>%
taxi_metrics(truth = tip, estimate = .pred_class)
set.seed(3214)
bootstraps(taxi_train)
taxi_res %>%
collect_metrics()
augment(tree_fit, new_data = taxi_train) %>%
roc_auc(truth = tip, .pred_yes)
taxi_res %>%
collect_metrics() %>%
select(.metric, mean, n)
taxi_metrics <- metric_set(accuracy, specificity, sensitivity)
augment(tree_fit, new_data = taxi_test) %>%
taxi_metrics(truth = tip, estimate = .pred_class)
augment(tree_fit, new_data = taxi_test) %>%
roc_auc(truth = tip, .pred_yes)
#| include: false
# might be best for logistic regression
# test_results <- tree_fit %>%
#   predict(taxi_test) %>%
#   bind_cols(taxi_test) %>%
#   metrics(truth = tip, estimate = .pred_class)
# test_results
# Extract the fitted rpart object
taxi_obj <- taxt_fit$fit
# Extract the fitted rpart object
taxi_obj <- taxi_fit$fit
# Extract the fitted rpart object
taxi_obj <- tree_fit$fit
# Extract the fitted rpart object
taxi_obj <- tree_fit$fit
# Calculate variable importance
vi <- vip(tree_obj, method = "permute", target = "tip", metric = "accuracy")
install.packages("vip")
library(vip)
# Extract the fitted rpart object
taxi_obj <- tree_fit$fit
# Calculate variable importance
vi <- vip(tree_obj, method = "permute", target = "tip", metric = "accuracy")
library(vip)
# Extract the fitted rpart object
taxi_obj <- tree_fit$fit
# Calculate variable importance
vi <- vip(taxi_obj, method = "permute", target = "tip", metric = "accuracy")
taxi_obj <- tree_fit$fit
taxi_obj
tree_fit
library(vip)
# Extract the fitted rpart object
# taxi_obj <- tree_fit$fit
# Calculate variable importance
vi <- vip(tree_fit, method = "permute", target = "tip", metric = "accuracy")
library(vip)
# Extract the fitted rpart object
taxi_obj <- tree_fit$fit
set.seed(123) # for reproducibility
vi <- vip(taxi_obj, method = "permute", target = "tip",
metric = "accuracy", train = taxi_train, nsim = 50)
library(vip)
# Extract the fitted rpart object
taxi_obj <- tree_fit$fit
set.seed(123) # for reproducibility
pred_wrapper <- function(model, newdata) {
predict(model, newdata, type = "prob")[, "yes"]
}
# Calculate variable importance using permutation with the defined wrapper
vi <- vip(tree_fit,
method = "permute",
target = "tip",
metric = "accuracy",
train = taxi_train,
nsim = 50,
pred_wrapper = pred_wrapper) # use the prediction wrapper
library(vip)
# Extract the fitted rpart object
taxi_obj <- tree_fit$fit
set.seed(123) # for reproducibility
pred_wrapper <- function(model, newdata) {
predict(model, newdata, type = "prob")[, "positive_class"]
}
# Calculate variable importance using permutation with the defined wrapper
vi <- vip(tree_fit,
method = "permute",
target = "tip",
metric = "roc_auc",
train = taxi_train,
nsim = 50,
pred_wrapper = pred_wrapper) # use the prediction wrapper
